{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import traceback\n",
    "import time\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql.functions import col, udf, array, count\n",
    "from pyspark.sql.functions import broadcast,coalesce, lit\n",
    "from itertools import chain\n",
    "from pyspark.sql.functions import col, lit, when, isnull\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OBJECTIVES:\n",
    "\n",
    "In this challenge, we would like to gather statistics on the number of parking violations (tickets) per street\n",
    "segment in NYC over the past 5 years. In particular, for each street segment in NYC, we would like to have the\n",
    "following:\n",
    "1. The total number of parking violations for each year from 2015 to 2019.\n",
    "2. The rate that the total number of violations change over the years using Ordinary Least Squares.\n",
    "\n",
    "The street address is provided through the House Number; Street Name; and Violation County field.\n",
    "For the parking violations data set, the Issue Date field should be used to determine which year a violationbelongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext()\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "streets = \"nyc_cscl.csv\"\n",
    "violations = \"nyc_parking_violation/*.csv\"\n",
    "\n",
    "# streets = \"hdfs:///tmp/bdm/nyc_cscl.csv\"\n",
    "# violations = \"hdfs:///tmp/bdm/nyc_parking_violations/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_upper(string):\n",
    "    if string is None:\n",
    "        return None\n",
    "    return string.strip().upper()\n",
    "\n",
    "def get_county_code(county):\n",
    "    if county is not None:\n",
    "        # Boro codes: 1 = MN, 2 = BX, 3 = BK, 4 = QN, 5 = SI\n",
    "        if county.startswith(\"M\") or county.startswith(\"N\"):\n",
    "            return 1\n",
    "        if county in ['BRONX', 'BX', 'PBX']:\n",
    "            return 2\n",
    "        if county in ['BK', 'K', 'KING', 'KINGS']:\n",
    "            return 3\n",
    "        if county.startswith('Q'):\n",
    "            return 4\n",
    "        if county == 'R' or county == 'ST':\n",
    "            return 5\n",
    "    return -1\n",
    "\n",
    "def get_year(string): \n",
    "    data_val = datetime.strptime(string.strip(), '%m/%d/%Y')    \n",
    "    return data_val.year\n",
    "\n",
    "def get_street_number(street_val):\n",
    "    if street_val is None:\n",
    "        return 0\n",
    "    if type(street_val) is int:\n",
    "        return street_val\n",
    "    elems = street_val.split(\"-\")\n",
    "    new_val = \"\".join(elems)\n",
    "    if new_val.isdigit():\n",
    "        return int(new_val)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def as_digit(val):\n",
    "    if val:\n",
    "        return int(val)\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_violations_df(violations_file, spark):\n",
    "    get_county_code_udf = udf(get_county_code)\n",
    "    get_street_number_udf = udf(get_street_number)\n",
    "    get_year_udf = udf(get_year)\n",
    "    to_upper_udf = udf(to_upper)\n",
    "    \n",
    "    violations_df = spark.read.csv(violations_file, header=True, inferSchema=True)\n",
    "\n",
    "    violations_df = violations_df.select(\"Violation County\", \"House Number\", \"Street Name\", \"Issue Date\")\n",
    "\n",
    "    violations_df = violations_df.filter((violations_df['Violation County'].isNotNull()) \n",
    "                                         & (violations_df['House Number'].isNotNull()) \n",
    "                                         & (violations_df['Street Name'].isNotNull()) \n",
    "                                         & (violations_df['Issue Date'].isNotNull())\n",
    "                                        )\n",
    "\n",
    "    violations_df = violations_df.withColumn('Violation County', get_county_code_udf(violations_df['Violation County']))\n",
    "    violations_df = violations_df.withColumn('House Number', get_street_number_udf(violations_df['House Number']))\n",
    "    violations_df = violations_df.withColumn('Street Name', to_upper_udf(violations_df['Street Name']))\n",
    "    violations_df = violations_df.withColumn('Issue Date', get_year_udf(violations_df['Issue Date']))\n",
    "\n",
    "    violations_df = violations_df.withColumnRenamed(\"Violation County\",\"COUNTY\")\n",
    "    violations_df = violations_df.withColumnRenamed(\"House Number\",\"HOUSENUM\")\n",
    "    violations_df = violations_df.withColumnRenamed(\"Street Name\",\"STREETNAME\")\n",
    "    violations_df = violations_df.withColumnRenamed(\"Issue Date\",\"YEAR\")\n",
    "\n",
    "    violations_df = violations_df.where(violations_df.YEAR.isin(list(range(2015,2020))))\n",
    "    violations_df = violations_df.repartition(5,'COUNTY')\n",
    "    violations_df = violations_df.alias('v')\n",
    "    return violations_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_streets_df(streets_file, spark):\n",
    "    get_street_number_udf = udf(get_street_number)\n",
    "    to_upper_udf = udf(to_upper)\n",
    "    as_digit_udf = udf(as_digit)\n",
    "    \n",
    "    streets_df = spark.read.csv(streets_file, header=True, inferSchema=True)\n",
    "\n",
    "    streets_df = streets_df.select(\"PHYSICALID\",\"BOROCODE\", \"FULL_STREE\", \"ST_LABEL\",\"L_LOW_HN\", \"L_HIGH_HN\", \n",
    "                                   \"R_LOW_HN\", \"R_HIGH_HN\")\n",
    "\n",
    "    streets_df = streets_df.withColumn('BOROCODE', as_digit_udf(streets_df['BOROCODE']))\n",
    "    streets_df = streets_df.withColumn('FULL_STREE', to_upper_udf(streets_df['FULL_STREE']))\n",
    "    streets_df = streets_df.withColumn('ST_LABEL',   to_upper_udf(streets_df['ST_LABEL']))\n",
    "    streets_df = streets_df.withColumn('L_LOW_HN',  get_street_number_udf(streets_df['L_LOW_HN']))\n",
    "    streets_df = streets_df.withColumn('L_HIGH_HN', get_street_number_udf(streets_df['L_HIGH_HN']))\n",
    "    streets_df = streets_df.withColumn('R_LOW_HN',  get_street_number_udf(streets_df['R_LOW_HN']))\n",
    "    streets_df = streets_df.withColumn('R_HIGH_HN', get_street_number_udf(streets_df['R_HIGH_HN']))\n",
    "\n",
    "    streets_df = streets_df.withColumnRenamed(\"L_LOW_HN\",\"OddLo\")\n",
    "    streets_df = streets_df.withColumnRenamed(\"L_HIGH_HN\",\"OddHi\")\n",
    "    streets_df = streets_df.withColumnRenamed(\"R_LOW_HN\",\"EvenLo\")\n",
    "    streets_df = streets_df.withColumnRenamed(\"R_HIGH_HN\",\"EvenHi\")\n",
    "    \n",
    "    streets_df = streets_df.repartition(5, 'BOROCODE')\n",
    "    streets_df = streets_df.alias('s')\n",
    "    return streets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "violations_df = get_violations_df(violations, spark)\n",
    "streets_df = get_streets_df(streets, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# violations_simp = pd.DataFrame(violations_df.head(5), columns=violations_df.columns)\n",
    "# violations_simp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# streets_simp = pd.DataFrame(streets_df.head(5), columns=streets_df.columns)\n",
    "# streets_simp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# streets_df = streets_df.alias('s')\n",
    "# violations_df = violations_df.alias('v')\n",
    "\n",
    "def mapper(row):\n",
    "    if row['FULL_STREE'] == row['ST_LABEL']:\n",
    "        yield ( \n",
    "                (row['BOROCODE'], row[\"FULL_STREE\"] ), \n",
    "                [( row['EvenLo'],row['EvenHi'],row['OddLo'],row['OddHi'], row['PHYSICALID'] )] \n",
    "              ) \n",
    "    else:\n",
    "        yield ( \n",
    "                (row['BOROCODE'], row[\"FULL_STREE\"]), \n",
    "                [( row['EvenLo'],row['EvenHi'],row['OddLo'],row['OddHi'] ,row['PHYSICALID'] )] \n",
    "              ) \n",
    "        yield ( \n",
    "                (row['BOROCODE'], row[\"ST_LABEL\"]), \n",
    "                [( row['EvenLo'],row['EvenHi'],row['OddLo'],row['OddHi'], row['PHYSICALID'] ) ]\n",
    "              ) \n",
    "        \n",
    "\n",
    "streets_dict = streets_df.rdd.flatMap(mapper).reduceByKey(lambda x,y: x+y).collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+------------------+----+----------+\n",
      "|COUNTY|HOUSENUM|        STREETNAME|YEAR|PHYSICALID|\n",
      "+------+--------+------------------+----+----------+\n",
      "|     4|    8027|       JAMAICA AVE|2015|      6471|\n",
      "|     4|    8246|          135TH ST|2015|     92936|\n",
      "|     4|    8246|          135TH ST|2015|     92936|\n",
      "|     4|   21447|       JAMAICA AVE|2015|      6471|\n",
      "|     4|    2260|           26TH ST|2015|     97676|\n",
      "|     4|    4018|           29TH ST|2015|     24685|\n",
      "|     4|    8909|          162ND ST|2015|     25168|\n",
      "|     4|    4202|COLLEGE POINT BLVD|2015|     29578|\n",
      "|     4|   15608|    CROSS BAY BLVD|2015|     73349|\n",
      "|     4|    4322|       QUEENS BLVD|2015|     12455|\n",
      "|     4|    3529|     FARRINGTON ST|2015|     32032|\n",
      "|     4|    4705|           45TH ST|2015|     83319|\n",
      "|     4|       0|     NORTHERN BLVD|2015|      9677|\n",
      "|     4|    9120|      ATLANTIC AVE|2015|      5467|\n",
      "|     4|   13355|     ROOSEVELT AVE|2015|    168241|\n",
      "|     4|   15922|          102ND ST|2015|      9866|\n",
      "|     4|   11117|       QUEENS BLVD|2015|     12438|\n",
      "|     4|    3513|       STEINWAY ST|2015|     21573|\n",
      "|     4|    3513|       STEINWAY ST|2015|     21573|\n",
      "|     4|    5860|           55TH DR|2015|     82085|\n",
      "+------+--------+------------------+----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_val(borocode, street, housenum):\n",
    "    val = streets_dict.get( (borocode, street) )\n",
    "    if val:\n",
    "        for item in val:\n",
    "            if int(housenum) % 2 == 0:\n",
    "                if int(item[0]) >= int(housenum )and int(housenum) <= int(item[1]):\n",
    "                    return item[4]\n",
    "            else:\n",
    "                if int(item[2]) >= int(housenum) and int(housenum) <= int(item[3]):\n",
    "                    return item[4]      \n",
    "    return None\n",
    "\n",
    "get_val_udf = udf(get_val)\n",
    "\n",
    "violations_2 = violations_df.withColumn('PHYSICALID', get_val_udf(violations_df['v.County'], \n",
    "                                                          violations_df['v.STREETNAME'], violations_df['v.HOUSENUM']\n",
    "                                                          ))\n",
    "\n",
    "violations_2 = violations_2.filter( violations_2['PHYSICALID'].isNotNull() )\n",
    "violations_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+----------+\n",
      "|PHYSICALID|YEAR|YEAR_COUNT|\n",
      "+----------+----+----------+\n",
      "|     23724|2015|         1|\n",
      "|     69833|2015|         1|\n",
      "|     43038|2015|         1|\n",
      "|      1900|2015|         1|\n",
      "|     96699|2015|         1|\n",
      "|     77522|2015|         1|\n",
      "|     81602|2015|         1|\n",
      "|    102112|2015|         1|\n",
      "|     32232|2015|         1|\n",
      "|     68066|2015|         1|\n",
      "+----------+----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "violations_2 = violations_2.groupBy(\"PHYSICALID\", \"YEAR\").agg(count(\"*\").alias(\"YEAR_COUNT\"))\n",
    "violations_2.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "violations_2.createOrReplaceTempView(\"violations2_results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+----------+----------+----------+\n",
      "|PHYSICALID|COUNT_2015|COUNT_2016|COUNT_2017|COUNT_2018|COUNT_2019|\n",
      "+----------+----------+----------+----------+----------+----------+\n",
      "|    100016|         1|         0|         0|         0|         0|\n",
      "|    100070|         1|         0|         0|         0|         0|\n",
      "|    100106|         1|         0|         0|         0|         0|\n",
      "|    100172|         1|         0|         0|         0|         0|\n",
      "|    100181|         1|         0|         0|         0|         0|\n",
      "|    100272|         1|         0|         0|         0|         0|\n",
      "|    100322|         1|         0|         0|         0|         0|\n",
      "|    100735|         1|         0|         0|         0|         0|\n",
      "|    100921|         1|         0|         0|         0|         0|\n",
      "|    100983|         1|         0|         0|         0|         0|\n",
      "+----------+----------+----------+----------+----------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summaries = spark.sql(\n",
    "    \"select PHYSICALID, \" +\n",
    "    \"MAX(CASE WHEN (YEAR = 2015) THEN YEAR_COUNT ELSE 0 END) AS COUNT_2015, \" +\n",
    "    \"MAX(CASE WHEN (YEAR = 2016) THEN YEAR_COUNT ELSE 0 END) AS COUNT_2016, \" +\n",
    "    \"MAX(CASE WHEN (YEAR = 2017) THEN YEAR_COUNT ELSE 0 END) AS COUNT_2017, \" +\n",
    "    \"MAX(CASE WHEN (YEAR = 2018) THEN YEAR_COUNT ELSE 0 END) AS COUNT_2018, \" +\n",
    "    \"MAX(CASE WHEN (YEAR = 2019) THEN YEAR_COUNT ELSE 0 END) AS COUNT_2019  \" +\n",
    "    \"from violations2_results \" +\n",
    "    \"group by PHYSICALID \" +\n",
    "    \"order by PHYSICALID \"\n",
    ")\n",
    "summaries.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+----------+----------+----------+--------+\n",
      "|PHYSICALID|COUNT_2015|COUNT_2016|COUNT_2017|COUNT_2018|COUNT_2019|OLS_COEF|\n",
      "+----------+----------+----------+----------+----------+----------+--------+\n",
      "|    100016|         1|         0|         0|         0|         0|     0.6|\n",
      "|    100070|         1|         0|         0|         0|         0|     0.6|\n",
      "|    100106|         1|         0|         0|         0|         0|     0.6|\n",
      "|    100172|         1|         0|         0|         0|         0|     0.6|\n",
      "|    100181|         1|         0|         0|         0|         0|     0.6|\n",
      "+----------+----------+----------+----------+----------+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summaries = summaries.withColumn('OLS_COEF', \n",
    "                getOLS_udf(array('COUNT_2015', 'COUNT_2016', 'COUNT_2017', 'COUNT_2018', 'COUNT_2019')))\n",
    "summaries = summaries.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = (\n",
    "    violations_df.join(\n",
    "        broadcast(streets_df),\n",
    "        ((col(\"s.BOROCODE\") == col(\"v.COUNTY\")) &\n",
    "        (\n",
    "            (col(\"s.FULL_STREE\") == col(\"v.STREETNAME\")) | \n",
    "            (col(\"s.ST_LABEL\") == col(\"v.STREETNAME\"))\n",
    "        ) &\n",
    "        (\n",
    "            ((col(\"v.HOUSENUM\") % 2 == 0)  & (col(\"v.HOUSENUM\") >= col(\"s.EvenLo\")) & (col(\"v.HOUSENUM\") <= col(\"s.EvenHi\"))) |  \n",
    "            ((col(\"v.HOUSENUM\") % 2 == 1)  & (col(\"v.HOUSENUM\") >= col(\"s.OddLo\"))  & (col(\"v.HOUSENUM\") <= col(\"s.OddHi\")))\n",
    "        )\n",
    "    ), how='inner')\n",
    ").select(col(\"s.PHYSICALID\"),col(\"v.YEAR\"))\n",
    "\n",
    "merged_df = merged_df.alias('m')\n",
    "merged_df = merged_df.groupBy(\"m.PHYSICALID\", \"m.YEAR\").agg(count(\"*\").alias(\"YEAR_COUNT\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+----------+\n",
      "|PHYSICALID|YEAR|YEAR_COUNT|\n",
      "+----------+----+----------+\n",
      "|     17275|2015|         1|\n",
      "|     73374|2015|         6|\n",
      "|     90616|2015|         1|\n",
      "|    127841|2015|         1|\n",
      "|    146935|2015|         2|\n",
      "|      8574|2015|         1|\n",
      "|     10729|2015|         2|\n",
      "|     90192|2015|         1|\n",
      "|     13798|2015|         8|\n",
      "|     71027|2015|         1|\n",
      "+----------+----+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "--- 234.21456098556519 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "merged_df.show(10)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "#  436.2948143482208 seconds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.createOrReplaceTempView(\"merged_results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"cannot resolve '`PHYSICALID`' given input columns: [m.COUNTY, m.STREETNAME, m.YEAR, m.HOUSENUM, m.ID]; line 1 pos 408;\\n'Sort ['PHYSICALID ASC NULLS FIRST], true\\n+- 'Aggregate ['PHYSICALID], ['m.PHYSICALID, 'MAX(CASE WHEN (cast(YEAR#1274 as int) = 2015) THEN 'YEAR_COUNT ELSE 0 END) AS COUNT_2015#2063, 'MAX(CASE WHEN (cast(YEAR#1274 as int) = 2016) THEN 'YEAR_COUNT ELSE 0 END) AS COUNT_2016#2064, 'MAX(CASE WHEN (cast(YEAR#1274 as int) = 2017) THEN 'YEAR_COUNT ELSE 0 END) AS COUNT_2017#2065, 'MAX(CASE WHEN (cast(YEAR#1274 as int) = 2018) THEN 'YEAR_COUNT ELSE 0 END) AS COUNT_2018#2066, 'MAX(CASE WHEN (cast(YEAR#1274 as int) = 2019) THEN 'YEAR_COUNT ELSE 0 END) AS COUNT_2019#2067]\\n   +- SubqueryAlias `m`\\n      +- SubqueryAlias `merged_results`\\n         +- Project [COUNTY#1259, HOUSENUM#1264, STREETNAME#1269, YEAR#1274, get_val(County#1259, STREETNAME#1269, HOUSENUM#1264) AS ID#1987]\\n            +- SubqueryAlias `v`\\n               +- RepartitionByExpression [COUNTY#1259], 5\\n                  +- Filter cast(YEAR#1274 as string) IN (cast(2015 as string),cast(2016 as string),cast(2017 as string),cast(2018 as string),cast(2019 as string))\\n                     +- Project [COUNTY#1259, HOUSENUM#1264, STREETNAME#1269, Issue Date#1254 AS YEAR#1274]\\n                        +- Project [COUNTY#1259, HOUSENUM#1264, Street Name#1248 AS STREETNAME#1269, Issue Date#1254]\\n                           +- Project [COUNTY#1259, House Number#1242 AS HOUSENUM#1264, Street Name#1248, Issue Date#1254]\\n                              +- Project [Violation County#1236 AS COUNTY#1259, House Number#1242, Street Name#1248, Issue Date#1254]\\n                                 +- Project [Violation County#1236, House Number#1242, Street Name#1248, get_year(Issue Date#1149) AS Issue Date#1254]\\n                                    +- Project [Violation County#1236, House Number#1242, to_upper(Street Name#1169) AS Street Name#1248, Issue Date#1149]\\n                                       +- Project [Violation County#1236, get_street_number(House Number#1168) AS House Number#1242, Street Name#1169, Issue Date#1149]\\n                                          +- Project [get_county_code(Violation County#1166) AS Violation County#1236, House Number#1168, Street Name#1169, Issue Date#1149]\\n                                             +- Filter (((isnotnull(Violation County#1166) && isnotnull(House Number#1168)) && isnotnull(Street Name#1169)) && isnotnull(Issue Date#1149))\\n                                                +- Project [Violation County#1166, House Number#1168, Street Name#1169, Issue Date#1149]\\n                                                   +- Relation[Summons Number#1145L,Plate ID#1146,Registration State#1147,Plate Type#1148,Issue Date#1149,Violation Code#1150,Vehicle Body Type#1151,Vehicle Make#1152,Issuing Agency#1153,Street Code1#1154,Street Code2#1155,Street Code3#1156,Vehicle Expiration Date#1157,Violation Location#1158,Violation Precinct#1159,Issuer Precinct#1160,Issuer Code#1161,Issuer Command#1162,Issuer Squad#1163,Violation Time#1164,Time First Observed#1165,Violation County#1166,Violation In Front Of Or Opposite#1167,House Number#1168,... 19 more fields] csv\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda2\\envs\\AppliedDataScience\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\AppliedDataScience\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o20.sql.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`PHYSICALID`' given input columns: [m.COUNTY, m.STREETNAME, m.YEAR, m.HOUSENUM, m.ID]; line 1 pos 408;\n'Sort ['PHYSICALID ASC NULLS FIRST], true\n+- 'Aggregate ['PHYSICALID], ['m.PHYSICALID, 'MAX(CASE WHEN (cast(YEAR#1274 as int) = 2015) THEN 'YEAR_COUNT ELSE 0 END) AS COUNT_2015#2063, 'MAX(CASE WHEN (cast(YEAR#1274 as int) = 2016) THEN 'YEAR_COUNT ELSE 0 END) AS COUNT_2016#2064, 'MAX(CASE WHEN (cast(YEAR#1274 as int) = 2017) THEN 'YEAR_COUNT ELSE 0 END) AS COUNT_2017#2065, 'MAX(CASE WHEN (cast(YEAR#1274 as int) = 2018) THEN 'YEAR_COUNT ELSE 0 END) AS COUNT_2018#2066, 'MAX(CASE WHEN (cast(YEAR#1274 as int) = 2019) THEN 'YEAR_COUNT ELSE 0 END) AS COUNT_2019#2067]\n   +- SubqueryAlias `m`\n      +- SubqueryAlias `merged_results`\n         +- Project [COUNTY#1259, HOUSENUM#1264, STREETNAME#1269, YEAR#1274, get_val(County#1259, STREETNAME#1269, HOUSENUM#1264) AS ID#1987]\n            +- SubqueryAlias `v`\n               +- RepartitionByExpression [COUNTY#1259], 5\n                  +- Filter cast(YEAR#1274 as string) IN (cast(2015 as string),cast(2016 as string),cast(2017 as string),cast(2018 as string),cast(2019 as string))\n                     +- Project [COUNTY#1259, HOUSENUM#1264, STREETNAME#1269, Issue Date#1254 AS YEAR#1274]\n                        +- Project [COUNTY#1259, HOUSENUM#1264, Street Name#1248 AS STREETNAME#1269, Issue Date#1254]\n                           +- Project [COUNTY#1259, House Number#1242 AS HOUSENUM#1264, Street Name#1248, Issue Date#1254]\n                              +- Project [Violation County#1236 AS COUNTY#1259, House Number#1242, Street Name#1248, Issue Date#1254]\n                                 +- Project [Violation County#1236, House Number#1242, Street Name#1248, get_year(Issue Date#1149) AS Issue Date#1254]\n                                    +- Project [Violation County#1236, House Number#1242, to_upper(Street Name#1169) AS Street Name#1248, Issue Date#1149]\n                                       +- Project [Violation County#1236, get_street_number(House Number#1168) AS House Number#1242, Street Name#1169, Issue Date#1149]\n                                          +- Project [get_county_code(Violation County#1166) AS Violation County#1236, House Number#1168, Street Name#1169, Issue Date#1149]\n                                             +- Filter (((isnotnull(Violation County#1166) && isnotnull(House Number#1168)) && isnotnull(Street Name#1169)) && isnotnull(Issue Date#1149))\n                                                +- Project [Violation County#1166, House Number#1168, Street Name#1169, Issue Date#1149]\n                                                   +- Relation[Summons Number#1145L,Plate ID#1146,Registration State#1147,Plate Type#1148,Issue Date#1149,Violation Code#1150,Vehicle Body Type#1151,Vehicle Make#1152,Issuing Agency#1153,Street Code1#1154,Street Code2#1155,Street Code3#1156,Vehicle Expiration Date#1157,Violation Location#1158,Violation Precinct#1159,Issuer Precinct#1160,Issuer Code#1161,Issuer Command#1162,Issuer Squad#1163,Violation Time#1164,Time First Observed#1165,Violation County#1166,Violation In Front Of Or Opposite#1167,House Number#1168,... 19 more fields] csv\n\r\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:111)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:108)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:280)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:280)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:279)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:121)\r\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\r\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\r\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:121)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:108)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:125)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:125)\r\n\tat scala.collection.immutable.List.foreach(List.scala:392)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:125)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:58)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:56)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:48)\r\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\r\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-68-a58113746bfc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;34m\"MAX(CASE WHEN (YEAR = 2019) THEN YEAR_COUNT ELSE 0 END) AS COUNT_2019  \"\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;34m\"from merged_results m  \"\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[1;34m\"group by PHYSICALID \"\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[1;34m\"order by PHYSICALID \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m )\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\AppliedDataScience\\lib\\site-packages\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36msql\u001b[1;34m(self, sqlQuery)\u001b[0m\n\u001b[0;32m    765\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'row1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'row2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'row3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m         \"\"\"\n\u001b[1;32m--> 767\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    768\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\AppliedDataScience\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\AppliedDataScience\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[0;32m     68\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: \"cannot resolve '`PHYSICALID`' given input columns: [m.COUNTY, m.STREETNAME, m.YEAR, m.HOUSENUM, m.ID]; line 1 pos 408;\\n'Sort ['PHYSICALID ASC NULLS FIRST], true\\n+- 'Aggregate ['PHYSICALID], ['m.PHYSICALID, 'MAX(CASE WHEN (cast(YEAR#1274 as int) = 2015) THEN 'YEAR_COUNT ELSE 0 END) AS COUNT_2015#2063, 'MAX(CASE WHEN (cast(YEAR#1274 as int) = 2016) THEN 'YEAR_COUNT ELSE 0 END) AS COUNT_2016#2064, 'MAX(CASE WHEN (cast(YEAR#1274 as int) = 2017) THEN 'YEAR_COUNT ELSE 0 END) AS COUNT_2017#2065, 'MAX(CASE WHEN (cast(YEAR#1274 as int) = 2018) THEN 'YEAR_COUNT ELSE 0 END) AS COUNT_2018#2066, 'MAX(CASE WHEN (cast(YEAR#1274 as int) = 2019) THEN 'YEAR_COUNT ELSE 0 END) AS COUNT_2019#2067]\\n   +- SubqueryAlias `m`\\n      +- SubqueryAlias `merged_results`\\n         +- Project [COUNTY#1259, HOUSENUM#1264, STREETNAME#1269, YEAR#1274, get_val(County#1259, STREETNAME#1269, HOUSENUM#1264) AS ID#1987]\\n            +- SubqueryAlias `v`\\n               +- RepartitionByExpression [COUNTY#1259], 5\\n                  +- Filter cast(YEAR#1274 as string) IN (cast(2015 as string),cast(2016 as string),cast(2017 as string),cast(2018 as string),cast(2019 as string))\\n                     +- Project [COUNTY#1259, HOUSENUM#1264, STREETNAME#1269, Issue Date#1254 AS YEAR#1274]\\n                        +- Project [COUNTY#1259, HOUSENUM#1264, Street Name#1248 AS STREETNAME#1269, Issue Date#1254]\\n                           +- Project [COUNTY#1259, House Number#1242 AS HOUSENUM#1264, Street Name#1248, Issue Date#1254]\\n                              +- Project [Violation County#1236 AS COUNTY#1259, House Number#1242, Street Name#1248, Issue Date#1254]\\n                                 +- Project [Violation County#1236, House Number#1242, Street Name#1248, get_year(Issue Date#1149) AS Issue Date#1254]\\n                                    +- Project [Violation County#1236, House Number#1242, to_upper(Street Name#1169) AS Street Name#1248, Issue Date#1149]\\n                                       +- Project [Violation County#1236, get_street_number(House Number#1168) AS House Number#1242, Street Name#1169, Issue Date#1149]\\n                                          +- Project [get_county_code(Violation County#1166) AS Violation County#1236, House Number#1168, Street Name#1169, Issue Date#1149]\\n                                             +- Filter (((isnotnull(Violation County#1166) && isnotnull(House Number#1168)) && isnotnull(Street Name#1169)) && isnotnull(Issue Date#1149))\\n                                                +- Project [Violation County#1166, House Number#1168, Street Name#1169, Issue Date#1149]\\n                                                   +- Relation[Summons Number#1145L,Plate ID#1146,Registration State#1147,Plate Type#1148,Issue Date#1149,Violation Code#1150,Vehicle Body Type#1151,Vehicle Make#1152,Issuing Agency#1153,Street Code1#1154,Street Code2#1155,Street Code3#1156,Vehicle Expiration Date#1157,Violation Location#1158,Violation Precinct#1159,Issuer Precinct#1160,Issuer Code#1161,Issuer Command#1162,Issuer Squad#1163,Violation Time#1164,Time First Observed#1165,Violation County#1166,Violation In Front Of Or Opposite#1167,House Number#1168,... 19 more fields] csv\\n\""
     ]
    }
   ],
   "source": [
    "summaries = spark.sql(\n",
    "    \"select m.PHYSICALID, \" +\n",
    "    \"MAX(CASE WHEN (YEAR = 2015) THEN YEAR_COUNT ELSE 0 END) AS COUNT_2015, \" +\n",
    "    \"MAX(CASE WHEN (YEAR = 2016) THEN YEAR_COUNT ELSE 0 END) AS COUNT_2016, \" +\n",
    "    \"MAX(CASE WHEN (YEAR = 2017) THEN YEAR_COUNT ELSE 0 END) AS COUNT_2017, \" +\n",
    "    \"MAX(CASE WHEN (YEAR = 2018) THEN YEAR_COUNT ELSE 0 END) AS COUNT_2018, \" +\n",
    "    \"MAX(CASE WHEN (YEAR = 2019) THEN YEAR_COUNT ELSE 0 END) AS COUNT_2019  \" +\n",
    "    \"from merged_results m  \" +\n",
    "    \"group by PHYSICALID \" +\n",
    "    \"order by PHYSICALID \"\n",
    ")\n",
    "summaries.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOLS(values):\n",
    "    import statsmodels.api as sm\n",
    "    X = sm.add_constant(np.arange(len(values)))\n",
    "    fit = sm.OLS(values, X).fit()\n",
    "    coef = fit.params[0]\n",
    "    return float(coef)\n",
    "\n",
    "getOLS_udf = udf(getOLS)\n",
    "\n",
    "# summaries = summaries.withColumn('OLS_COEF', \n",
    "#                 getOLS_udf(array('COUNT_2015', 'COUNT_2016', 'COUNT_2017', 'COUNT_2018', 'COUNT_2019')))\n",
    "# summaries = summaries.alias('f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+----------+----------+----------+------------------+\n",
      "|PHYSICALID|COUNT_2015|COUNT_2016|COUNT_2017|COUNT_2018|COUNT_2019|          OLS_COEF|\n",
      "+----------+----------+----------+----------+----------+----------+------------------+\n",
      "|        29|         5|         0|         0|         0|         0|               3.0|\n",
      "|        30|         5|         0|         0|         0|         0|               3.0|\n",
      "|        50|         8|         0|         0|         0|         0|               4.8|\n",
      "|        58|         5|         0|         0|         0|         0|               3.0|\n",
      "|        62|         6|         0|         0|         0|         0|3.5999999999999996|\n",
      "|        66|         6|         0|         0|         0|         0|3.5999999999999996|\n",
      "|        67|        26|         0|         0|         0|         0|              15.6|\n",
      "|       116|         8|         0|         0|         0|         0|               4.8|\n",
      "|       120|         4|         0|         0|         0|         0|               2.4|\n",
      "|       125|         8|         0|         0|         0|         0|               4.8|\n",
      "|       126|         8|         0|         0|         0|         0|               4.8|\n",
      "|       129|         3|         0|         0|         0|         0|1.7999999999999998|\n",
      "|       131|         3|         0|         0|         0|         0|1.7999999999999998|\n",
      "|       132|         3|         0|         0|         0|         0|1.7999999999999998|\n",
      "|       134|        20|         0|         0|         0|         0|              12.0|\n",
      "|       139|         4|         0|         0|         0|         0|               2.4|\n",
      "|       140|         4|         0|         0|         0|         0|               2.4|\n",
      "|       142|         1|         0|         0|         0|         0|               0.6|\n",
      "|       143|        10|         0|         0|         0|         0|               6.0|\n",
      "|       145|         5|         0|         0|         0|         0|               3.0|\n",
      "+----------+----------+----------+----------+----------+----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "summaries.show()\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "streets_df = streets_df.select(col(\"s.PHYSICALID\")) \\\n",
    "                    .join(summaries, \"PHYSICALID\", how='left') \\\n",
    "                    .distinct() \\\n",
    "                    .orderBy(\"PHYSICALID\") \\\n",
    "\n",
    "streets_df = streets_df.withColumn(\"COUNT_2015\",coalesce(\"COUNT_2015\", lit(0))) \n",
    "streets_df = streets_df.withColumn(\"COUNT_2016\",coalesce(\"COUNT_2016\", lit(0))) \n",
    "streets_df = streets_df.withColumn(\"COUNT_2017\",coalesce(\"COUNT_2017\", lit(0))) \n",
    "streets_df = streets_df.withColumn(\"COUNT_2018\",coalesce(\"COUNT_2018\", lit(0))) \n",
    "streets_df = streets_df.withColumn(\"COUNT_2019\",coalesce(\"COUNT_2019\", lit(0))) \n",
    "streets_df = streets_df.withColumn(\"OLS_COEF\",  coalesce(\"OLS_COEF\", lit(0.0))) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+----------+----------+----------+--------+\n",
      "|PHYSICALID|COUNT_2015|COUNT_2016|COUNT_2017|COUNT_2018|COUNT_2019|OLS_COEF|\n",
      "+----------+----------+----------+----------+----------+----------+--------+\n",
      "|         3|         0|         0|         0|         0|         0|     0.0|\n",
      "|         5|         0|         0|         0|         0|         0|     0.0|\n",
      "|         6|         0|         0|         0|         0|         0|     0.0|\n",
      "|         8|         0|         0|         0|         0|         0|     0.0|\n",
      "|        14|         0|         0|         0|         0|         0|     0.0|\n",
      "|        23|         0|         0|         0|         0|         0|     0.0|\n",
      "|        24|         0|         0|         0|         0|         0|     0.0|\n",
      "|        25|         0|         0|         0|         0|         0|     0.0|\n",
      "|        29|         5|         0|         0|         0|         0|     3.0|\n",
      "|        30|         5|         0|         0|         0|         0|     3.0|\n",
      "|        33|         0|         0|         0|         0|         0|     0.0|\n",
      "|        34|         0|         0|         0|         0|         0|     0.0|\n",
      "|        36|         0|         0|         0|         0|         0|     0.0|\n",
      "|        37|         0|         0|         0|         0|         0|     0.0|\n",
      "|        40|         0|         0|         0|         0|         0|     0.0|\n",
      "|        41|         0|         0|         0|         0|         0|     0.0|\n",
      "|        43|         0|         0|         0|         0|         0|     0.0|\n",
      "|        44|         0|         0|         0|         0|         0|     0.0|\n",
      "|        45|         0|         0|         0|         0|         0|     0.0|\n",
      "|        46|         0|         0|         0|         0|         0|     0.0|\n",
      "+----------+----------+----------+----------+----------+----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "streets_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# streets_df.write.csv('TODO', header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Path:  output\n",
      "+----------+----------+----------+----------+----------+----------+--------+\n",
      "|PHYSICALID|COUNT_2015|COUNT_2016|COUNT_2017|COUNT_2018|COUNT_2019|OLS_COEF|\n",
      "+----------+----------+----------+----------+----------+----------+--------+\n",
      "|         3|         0|         0|         0|         0|         0|     0.0|\n",
      "|         5|         0|         0|         0|         0|         0|     0.0|\n",
      "|         6|         0|         0|         0|         0|         0|     0.0|\n",
      "|         8|         0|         0|         0|         0|         0|     0.0|\n",
      "|        14|         0|         0|         0|         0|         0|     0.0|\n",
      "|        23|         0|         0|         0|         0|         0|     0.0|\n",
      "|        24|         0|         0|         0|         0|         0|     0.0|\n",
      "|        25|         0|         0|         0|         0|         0|     0.0|\n",
      "|        29|         5|         0|         0|         0|         0|     3.0|\n",
      "|        30|         5|         0|         0|         0|         0|     3.0|\n",
      "+----------+----------+----------+----------+----------+----------+--------+\n",
      "only showing top 10 rows\n",
      "\n",
      "None\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import BDM_Final\n",
    "start_time = time.time()\n",
    "%run -i BDM_Final.py output\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
