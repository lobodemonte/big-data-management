{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import traceback\n",
    "import time\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql.functions import col, udf, array, count\n",
    "from pyspark.sql.functions import broadcast,coalesce, lit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OBJECTIVES:\n",
    "\n",
    "In this challenge, we would like to gather statistics on the number of parking violations (tickets) per street\n",
    "segment in NYC over the past 5 years. In particular, for each street segment in NYC, we would like to have the\n",
    "following:\n",
    "1. The total number of parking violations for each year from 2015 to 2019.\n",
    "2. The rate that the total number of violations change over the years using Ordinary Least Squares.\n",
    "\n",
    "The street address is provided through the House Number; Street Name; and Violation County field.\n",
    "For the parking violations data set, the Issue Date field should be used to determine which year a violationbelongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext()\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "streets = \"nyc_cscl.csv\"\n",
    "violations = \"nyc_parking_violation/*.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# streets = \"hdfs:///tmp/bdm/nyc_cscl.csv\"\n",
    "# violations = \"hdfs:///tmp/bdm/nyc_parking_violations/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_upper(string):\n",
    "    if string is None:\n",
    "        return None\n",
    "    return string.strip().upper()\n",
    "\n",
    "def get_county_code(county):\n",
    "    if county is not None:\n",
    "        # Boro codes: 1 = MN, 2 = BX, 3 = BK, 4 = QN, 5 = SI\n",
    "        if county.startswith(\"M\") or county.startswith(\"N\"):\n",
    "            return 1\n",
    "        if county in ['BRONX', 'BX', 'PBX']:\n",
    "            return 2\n",
    "        if county in ['BK', 'K', 'KING', 'KINGS']:\n",
    "            return 3\n",
    "        if county.startswith('Q'):\n",
    "            return 4\n",
    "        if county == 'R' or county == 'ST':\n",
    "            return 5\n",
    "    return -1\n",
    "\n",
    "def get_year(string): \n",
    "    data_val = datetime.strptime(string.strip(), '%m/%d/%Y')    \n",
    "    return data_val.year\n",
    "\n",
    "def get_house_number(house_val):\n",
    "    if house_val is None:\n",
    "        return None\n",
    "    if type(house_val) is int:\n",
    "        return house_val\n",
    "    elems = house_val.split(\"-\")\n",
    "    new_val = \"\".join(elems)\n",
    "    if new_val.isdigit():\n",
    "        return int(new_val)\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def get_street_number(street_val):\n",
    "    if street_val is None:\n",
    "        return 0\n",
    "    if type(street_val) is int:\n",
    "        return street_val\n",
    "    elems = street_val.split(\"-\")\n",
    "    new_val = \"\".join(elems)\n",
    "    if new_val.isdigit():\n",
    "        return int(new_val)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "get_street_number_udf = udf(get_street_number)\n",
    "get_county_code_udf = udf(get_county_code)\n",
    "get_year_udf = udf(get_year)\n",
    "to_upper_udf = udf(to_upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_violations_df(violations_file, spark):\n",
    "    violations_df = spark.read.csv(violations_file,header=True, inferSchema=True)\n",
    "\n",
    "    violations_df = violations_df.select(\"Violation County\", \"House Number\", \"Street Name\", \"Issue Date\")\n",
    "\n",
    "    violations_df = violations_df.filter((violations_df['Violation County'].isNotNull()) \n",
    "                                         & (violations_df['House Number'].isNotNull()) \n",
    "                                         & (violations_df['Street Name'].isNotNull()) \n",
    "                                         & (violations_df['Issue Date'].isNotNull())\n",
    "                                        )\n",
    "\n",
    "    violations_df = violations_df.withColumn('Violation County', get_county_code_udf(violations_df['Violation County']))\n",
    "    violations_df = violations_df.withColumn('House Number', get_street_number_udf(violations_df['House Number']))\n",
    "    violations_df = violations_df.withColumn('Street Name', to_upper_udf(violations_df['Street Name']))\n",
    "    violations_df = violations_df.withColumn('Issue Date', get_year_udf(violations_df['Issue Date']))\n",
    "\n",
    "    violations_df = violations_df.withColumnRenamed(\"Violation County\",\"COUNTY\")\n",
    "    violations_df = violations_df.withColumnRenamed(\"House Number\",\"HOUSENUM\")\n",
    "    violations_df = violations_df.withColumnRenamed(\"Street Name\",\"STREETNAME\")\n",
    "    violations_df = violations_df.withColumnRenamed(\"Issue Date\",\"YEAR\")\n",
    "\n",
    "    violations_df = violations_df.where(violations_df.YEAR.isin(list(range(2015,2020))))\n",
    "    violations_df = violations_df.repartition(5,'COUNTY')\n",
    "    return violations_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_streets_df(streets_file, spark):\n",
    "\n",
    "    streets_df = spark.read.csv(streets_file, header=True, inferSchema=True)\n",
    "\n",
    "    streets_df = streets_df.select(\"PHYSICALID\",\"BOROCODE\", \"FULL_STREE\", \"ST_LABEL\",\"L_LOW_HN\", \"L_HIGH_HN\", \n",
    "                                   \"R_LOW_HN\", \"R_HIGH_HN\")\n",
    "\n",
    "    streets_df = streets_df.withColumn('FULL_STREE', to_upper_udf(streets_df['FULL_STREE']))\n",
    "    streets_df = streets_df.withColumn('ST_LABEL',   to_upper_udf(streets_df['ST_LABEL']))\n",
    "    streets_df = streets_df.withColumn('L_LOW_HN',  get_street_number_udf(streets_df['L_LOW_HN']))\n",
    "    streets_df = streets_df.withColumn('L_HIGH_HN', get_street_number_udf(streets_df['L_HIGH_HN']))\n",
    "    streets_df = streets_df.withColumn('R_LOW_HN',  get_street_number_udf(streets_df['R_LOW_HN']))\n",
    "    streets_df = streets_df.withColumn('R_HIGH_HN', get_street_number_udf(streets_df['R_HIGH_HN']))\n",
    "\n",
    "    streets_df = streets_df.withColumnRenamed(\"L_LOW_HN\",\"OddLo\")\n",
    "    streets_df = streets_df.withColumnRenamed(\"L_HIGH_HN\",\"OddHi\")\n",
    "    streets_df = streets_df.withColumnRenamed(\"R_LOW_HN\",\"EvenLo\")\n",
    "    streets_df = streets_df.withColumnRenamed(\"R_HIGH_HN\",\"EvenHi\")\n",
    "    \n",
    "    streets_df = streets_df.repartition(5, 'BOROCODE')\n",
    "    return streets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "violations_df = get_violations_df(violations, spark)\n",
    "streets_df = get_streets_df(streets, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# violations_simp = pd.DataFrame(violations_df.head(5), columns=violations_df.columns)\n",
    "# violations_simp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# streets_simp = pd.DataFrame(streets_df.head(5), columns=streets_df.columns)\n",
    "# streets_simp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "streets_df = streets_df.alias('s')\n",
    "violations_df = violations_df.alias('v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = (\n",
    "    violations_df.join(\n",
    "        broadcast(streets_df),\n",
    "        ((col(\"s.BOROCODE\") == col(\"v.COUNTY\")) &\n",
    "        (\n",
    "            (col(\"s.FULL_STREE\") == col(\"v.STREETNAME\")) | \n",
    "            (col(\"s.ST_LABEL\") == col(\"v.STREETNAME\"))\n",
    "        ) &\n",
    "        (\n",
    "            ((col(\"v.HOUSENUM\") % 2 == 0)  & (col(\"v.HOUSENUM\") >= col(\"s.EvenLo\")) & (col(\"v.HOUSENUM\") <= col(\"s.EvenHi\"))) |  \n",
    "            ((col(\"v.HOUSENUM\") % 2 == 1)  & (col(\"v.HOUSENUM\") >= col(\"s.OddLo\"))  & (col(\"v.HOUSENUM\") <= col(\"s.OddHi\")))\n",
    "        )\n",
    "    ), how='inner')\n",
    ").select(col(\"s.PHYSICALID\"),col(\"v.YEAR\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.alias('m')\n",
    "merged_df = merged_df.groupBy(\"m.PHYSICALID\", \"m.YEAR\").agg(count(\"*\").alias(\"YEAR_COUNT\"))\n",
    "# merged_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.createOrReplaceTempView(\"merged_results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+----------+----------+----------+\n",
      "|PHYSICALID|COUNT_2015|COUNT_2016|COUNT_2017|COUNT_2018|COUNT_2019|\n",
      "+----------+----------+----------+----------+----------+----------+\n",
      "|        29|         5|         0|         0|         0|         0|\n",
      "|        30|         5|         0|         0|         0|         0|\n",
      "|        50|         8|         0|         0|         0|         0|\n",
      "|        58|         5|         0|         0|         0|         0|\n",
      "|        62|         6|         0|         0|         0|         0|\n",
      "|        66|         6|         0|         0|         0|         0|\n",
      "|        67|        26|         0|         0|         0|         0|\n",
      "|       116|         8|         0|         0|         0|         0|\n",
      "|       120|         4|         0|         0|         0|         0|\n",
      "|       125|         8|         0|         0|         0|         0|\n",
      "+----------+----------+----------+----------+----------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summaries = spark.sql(\n",
    "    \"select m.PHYSICALID, \" +\n",
    "    \"MAX(CASE WHEN (YEAR = 2015) THEN YEAR_COUNT ELSE 0 END) AS COUNT_2015, \" +\n",
    "    \"MAX(CASE WHEN (YEAR = 2016) THEN YEAR_COUNT ELSE 0 END) AS COUNT_2016, \" +\n",
    "    \"MAX(CASE WHEN (YEAR = 2017) THEN YEAR_COUNT ELSE 0 END) AS COUNT_2017, \" +\n",
    "    \"MAX(CASE WHEN (YEAR = 2018) THEN YEAR_COUNT ELSE 0 END) AS COUNT_2018, \" +\n",
    "    \"MAX(CASE WHEN (YEAR = 2019) THEN YEAR_COUNT ELSE 0 END) AS COUNT_2019  \" +\n",
    "    \"from merged_results m  \" +\n",
    "    \"group by m.PHYSICALID \" +\n",
    "    \"order by m.PHYSICALID \"\n",
    ")\n",
    "summaries.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOLS(values):\n",
    "    import statsmodels.api as sm\n",
    "    X = sm.add_constant(np.arange(len(values)))\n",
    "    fit = sm.OLS(values, X).fit()\n",
    "    coef = fit.params[0]\n",
    "    return float(coef)\n",
    "\n",
    "getOLS_udf = udf(getOLS)\n",
    "\n",
    "summaries = summaries.withColumn('OLS_COEF', \n",
    "                getOLS_udf(array('COUNT_2015', 'COUNT_2016', 'COUNT_2017', 'COUNT_2018', 'COUNT_2019')))\n",
    "summaries = summaries.alias('f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+----------+----------+----------+------------------+\n",
      "|PHYSICALID|COUNT_2015|COUNT_2016|COUNT_2017|COUNT_2018|COUNT_2019|          OLS_COEF|\n",
      "+----------+----------+----------+----------+----------+----------+------------------+\n",
      "|        29|         5|         0|         0|         0|         0|               3.0|\n",
      "|        30|         5|         0|         0|         0|         0|               3.0|\n",
      "|        50|         8|         0|         0|         0|         0|               4.8|\n",
      "|        58|         5|         0|         0|         0|         0|               3.0|\n",
      "|        62|         6|         0|         0|         0|         0|3.5999999999999996|\n",
      "|        66|         6|         0|         0|         0|         0|3.5999999999999996|\n",
      "|        67|        26|         0|         0|         0|         0|              15.6|\n",
      "|       116|         8|         0|         0|         0|         0|               4.8|\n",
      "|       120|         4|         0|         0|         0|         0|               2.4|\n",
      "|       125|         8|         0|         0|         0|         0|               4.8|\n",
      "|       126|         8|         0|         0|         0|         0|               4.8|\n",
      "|       129|         3|         0|         0|         0|         0|1.7999999999999998|\n",
      "|       131|         3|         0|         0|         0|         0|1.7999999999999998|\n",
      "|       132|         3|         0|         0|         0|         0|1.7999999999999998|\n",
      "|       134|        20|         0|         0|         0|         0|              12.0|\n",
      "|       139|         4|         0|         0|         0|         0|               2.4|\n",
      "|       140|         4|         0|         0|         0|         0|               2.4|\n",
      "|       142|         1|         0|         0|         0|         0|               0.6|\n",
      "|       143|        10|         0|         0|         0|         0|               6.0|\n",
      "|       145|         5|         0|         0|         0|         0|               3.0|\n",
      "+----------+----------+----------+----------+----------+----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summaries.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "streets_df = streets_df.select(col(\"s.PHYSICALID\")) \\\n",
    "                    .join(summaries, \"PHYSICALID\", how='left') \\\n",
    "                    .distinct() \\\n",
    "                    .orderBy(\"PHYSICALID\") \\\n",
    "\n",
    "streets_df = streets_df.withColumn(\"COUNT_2015\",coalesce(\"COUNT_2015\", lit(0))) \n",
    "streets_df = streets_df.withColumn(\"COUNT_2016\",coalesce(\"COUNT_2016\", lit(0))) \n",
    "streets_df = streets_df.withColumn(\"COUNT_2017\",coalesce(\"COUNT_2017\", lit(0))) \n",
    "streets_df = streets_df.withColumn(\"COUNT_2018\",coalesce(\"COUNT_2018\", lit(0))) \n",
    "streets_df = streets_df.withColumn(\"COUNT_2019\",coalesce(\"COUNT_2019\", lit(0))) \n",
    "streets_df = streets_df.withColumn(\"OLS_COEF\",  coalesce(\"OLS_COEF\", lit(0.0))) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+----------+----------+----------+--------+\n",
      "|PHYSICALID|COUNT_2015|COUNT_2016|COUNT_2017|COUNT_2018|COUNT_2019|OLS_COEF|\n",
      "+----------+----------+----------+----------+----------+----------+--------+\n",
      "|         3|         0|         0|         0|         0|         0|     0.0|\n",
      "|         5|         0|         0|         0|         0|         0|     0.0|\n",
      "|         6|         0|         0|         0|         0|         0|     0.0|\n",
      "|         8|         0|         0|         0|         0|         0|     0.0|\n",
      "|        14|         0|         0|         0|         0|         0|     0.0|\n",
      "|        23|         0|         0|         0|         0|         0|     0.0|\n",
      "|        24|         0|         0|         0|         0|         0|     0.0|\n",
      "|        25|         0|         0|         0|         0|         0|     0.0|\n",
      "|        29|         5|         0|         0|         0|         0|     3.0|\n",
      "|        30|         5|         0|         0|         0|         0|     3.0|\n",
      "|        33|         0|         0|         0|         0|         0|     0.0|\n",
      "|        34|         0|         0|         0|         0|         0|     0.0|\n",
      "|        36|         0|         0|         0|         0|         0|     0.0|\n",
      "|        37|         0|         0|         0|         0|         0|     0.0|\n",
      "|        40|         0|         0|         0|         0|         0|     0.0|\n",
      "|        41|         0|         0|         0|         0|         0|     0.0|\n",
      "|        43|         0|         0|         0|         0|         0|     0.0|\n",
      "|        44|         0|         0|         0|         0|         0|     0.0|\n",
      "|        45|         0|         0|         0|         0|         0|     0.0|\n",
      "|        46|         0|         0|         0|         0|         0|     0.0|\n",
      "+----------+----------+----------+----------+----------+----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "streets_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 554.238463640213 seconds ---\n"
     ]
    }
   ],
   "source": [
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# streets_df.write.csv('TODO', header=False)\n",
    "\n",
    "import BDM_Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Path:  output\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at C:\\Users\\erikl\\OneDrive\\Documents\\NYU_CUSP\\Spring2020\\BigDataMA\\big-data-management\\Final\\BDM_Final.py:143 ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\OneDrive\\Documents\\NYU_CUSP\\Spring2020\\BigDataMA\\big-data-management\\Final\\BDM_Final.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Output Path: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 205\u001b[1;33m     \u001b[0mrun_spark\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    206\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Done\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\Documents\\NYU_CUSP\\Spring2020\\BigDataMA\\big-data-management\\Final\\BDM_Final.py\u001b[0m in \u001b[0;36mrun_spark\u001b[1;34m(output_path)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mrun_spark\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 152\u001b[1;33m     \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    153\u001b[0m     \u001b[0mspark\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\AppliedDataScience\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    131\u001b[0m                     \" note this option will be removed in Spark 3.0\")\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m         \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\AppliedDataScience\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    330\u001b[0m                         \u001b[1;34m\" created by %s at %s:%s \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[1;32m--> 332\u001b[1;33m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[0;32m    333\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at C:\\Users\\erikl\\OneDrive\\Documents\\NYU_CUSP\\Spring2020\\BigDataMA\\big-data-management\\Final\\BDM_Final.py:143 "
     ]
    }
   ],
   "source": [
    "%run -i BDM_Final.py output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
